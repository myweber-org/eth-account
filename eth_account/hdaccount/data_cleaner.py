
import pandas as pd
import numpy as np
from scipy import stats

def remove_outliers_iqr(df, columns):
    cleaned_df = df.copy()
    for col in columns:
        if col in cleaned_df.columns:
            Q1 = cleaned_df[col].quantile(0.25)
            Q3 = cleaned_df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            cleaned_df = cleaned_df[(cleaned_df[col] >= lower_bound) & (cleaned_df[col] <= upper_bound)]
    return cleaned_df

def normalize_data(df, columns, method='minmax'):
    normalized_df = df.copy()
    for col in columns:
        if col in normalized_df.columns:
            if method == 'minmax':
                min_val = normalized_df[col].min()
                max_val = normalized_df[col].max()
                if max_val != min_val:
                    normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
                else:
                    normalized_df[col] = 0
            elif method == 'zscore':
                mean_val = normalized_df[col].mean()
                std_val = normalized_df[col].std()
                if std_val != 0:
                    normalized_df[col] = (normalized_df[col] - mean_val) / std_val
                else:
                    normalized_df[col] = 0
    return normalized_df

def clean_dataset(df, numeric_columns):
    df_cleaned = df.dropna(subset=numeric_columns)
    df_no_outliers = remove_outliers_iqr(df_cleaned, numeric_columns)
    df_normalized = normalize_data(df_no_outliers, numeric_columns, method='zscore')
    return df_normalized

if __name__ == "__main__":
    sample_data = {
        'feature_a': [1, 2, 3, 4, 5, 100, 6, 7, 8, 9],
        'feature_b': [10, 20, 30, 40, 50, 60, 70, 80, 90, 1000],
        'category': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B']
    }
    df = pd.DataFrame(sample_data)
    numeric_cols = ['feature_a', 'feature_b']
    
    cleaned_df = clean_dataset(df, numeric_cols)
    print("Original dataset shape:", df.shape)
    print("Cleaned dataset shape:", cleaned_df.shape)
    print("\nCleaned data summary:")
    print(cleaned_df.describe())
import numpy as np

def remove_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    filtered_data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]
    return filtered_data
import pandas as pd
import numpy as np

def clean_dataset(df, column_mapping=None, drop_duplicates=True, fill_missing=True):
    """
    Clean a pandas DataFrame by standardizing columns, removing duplicates,
    and handling missing values.
    """
    cleaned_df = df.copy()
    
    if column_mapping:
        cleaned_df = cleaned_df.rename(columns=column_mapping)
    
    if drop_duplicates:
        cleaned_df = cleaned_df.drop_duplicates()
    
    if fill_missing:
        for col in cleaned_df.columns:
            if cleaned_df[col].dtype in ['int64', 'float64']:
                cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].median())
            elif cleaned_df[col].dtype == 'object':
                cleaned_df[col] = cleaned_df[col].fillna('Unknown')
    
    cleaned_df = cleaned_df.reset_index(drop=True)
    return cleaned_df

def validate_data(df, required_columns=None, unique_constraints=None):
    """
    Validate data integrity by checking required columns and unique constraints.
    """
    validation_report = {}
    
    if required_columns:
        missing_columns = [col for col in required_columns if col not in df.columns]
        validation_report['missing_columns'] = missing_columns
    
    if unique_constraints:
        duplicate_counts = {}
        for constraint in unique_constraints:
            if isinstance(constraint, str):
                if constraint in df.columns:
                    duplicates = df[df.duplicated(subset=[constraint], keep=False)]
                    duplicate_counts[constraint] = len(duplicates)
            elif isinstance(constraint, list):
                if all(col in df.columns for col in constraint):
                    duplicates = df[df.duplicated(subset=constraint, keep=False)]
                    duplicate_counts[tuple(constraint)] = len(duplicates)
        validation_report['duplicate_violations'] = duplicate_counts
    
    return validation_report

def standardize_text_columns(df, columns):
    """
    Standardize text columns by converting to lowercase and stripping whitespace.
    """
    standardized_df = df.copy()
    
    for col in columns:
        if col in standardized_df.columns:
            standardized_df[col] = standardized_df[col].astype(str).str.lower().str.strip()
    
    return standardized_df

if __name__ == "__main__":
    sample_data = {
        'Name': ['Alice', 'Bob', 'Alice', 'Charlie', None],
        'Age': [25, 30, 25, 35, 40],
        'Email': ['alice@test.com', 'bob@test.com', 'alice@test.com', 'charlie@test.com', 'david@test.com']
    }
    
    df = pd.DataFrame(sample_data)
    print("Original DataFrame:")
    print(df)
    print("\n")
    
    cleaned = clean_dataset(df, column_mapping={'Name': 'Full_Name'})
    print("Cleaned DataFrame:")
    print(cleaned)
    print("\n")
    
    validation = validate_data(cleaned, required_columns=['Full_Name', 'Age'], unique_constraints=['Email'])
    print("Validation Report:")
    print(validation)
    print("\n")
    
    standardized = standardize_text_columns(cleaned, ['Full_Name', 'Email'])
    print("Standardized DataFrame:")
    print(standardized)